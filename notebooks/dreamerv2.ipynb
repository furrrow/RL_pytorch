{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22034,
     "status": "ok",
     "timestamp": 1626729724753,
     "user": {
      "displayName": "Tony Schroeder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilICEJmMvke-YcAXapHV2CDnsfjBfDiGICD336=s64",
      "userId": "03301841124230681313"
     },
     "user_tz": 420
    },
    "id": "G-B4oPoyAerT",
    "outputId": "d5bc9d0a-aed0-4c3f-9b10-f3859016f5e5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28012,
     "status": "ok",
     "timestamp": 1626729752755,
     "user": {
      "displayName": "Tony Schroeder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilICEJmMvke-YcAXapHV2CDnsfjBfDiGICD336=s64",
      "userId": "03301841124230681313"
     },
     "user_tz": 420
    },
    "id": "OoIImrtHBfT9",
    "outputId": "aa4c0439-b158-4515-c119-30426f11589f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
      "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
      "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
      "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
      "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
      "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
      "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
      "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
      "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
      "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
      "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
      "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
      "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
      "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
      "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
      "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
      "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
      "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
      "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
      "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
      "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
      "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
      "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
      "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
      "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
      "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
      "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
      "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
      "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
      "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
      "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
      "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
      "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
      "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
      "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
      "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
      "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
      "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
      "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
      "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
      "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
      "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
      "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
      "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
      "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
      "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
      "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
      "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
      "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
      "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
      "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
      "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
      "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
      "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
      "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
      "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
      "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
      "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
      "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
      "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
      "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
      "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
      "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
      "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
      "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
      "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
      "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
      "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
      "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
      "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
      "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
      "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
      "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
      "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
      "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
      "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/atari_py/import_roms.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/atari_py/import_roms.py\", line 89, in main\n",
      "    import_roms(args.dirpath)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/atari_py/import_roms.py\", line 82, in import_roms\n",
      "    save_if_matches(f)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/atari_py/import_roms.py\", line 48, in save_if_matches\n",
      "    hexdigest = _calc_md5(f)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/atari_py/import_roms.py\", line 29, in _calc_md5\n",
      "    chunk = f.read(MD5_CHUNK_SIZE)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m atari_py.import_roms /content/drive/MyDrive/GT/DL/atari/roms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1626741554414,
     "user": {
      "displayName": "Tony Schroeder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilICEJmMvke-YcAXapHV2CDnsfjBfDiGICD336=s64",
      "userId": "03301841124230681313"
     },
     "user_tz": 420
    },
    "id": "jbHaLWgOOFOo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions.kl import kl_divergence\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize\n",
    "import gym\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            stride (int): Controls the stride.\n",
    "        \"\"\"\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1).to(device),\n",
    "            nn.BatchNorm2d(out_channels).to(device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1).to(device),\n",
    "            nn.BatchNorm2d(out_channels).to(device),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=3, stride=2, return_indices=True).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        # x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_blocks = (\n",
    "            ConvBlock(3, 8, device),\n",
    "            ConvBlock(8, 16, device),\n",
    "            ConvBlock(16, 32, device),\n",
    "            ConvBlock(32, 64, device),\n",
    "            # ConvBlock(64, 128, device),\n",
    "            # ConvBlock(128, 256, device),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "        return self.flatten(x)\n",
    "        # indices = []\n",
    "        # for conv_block in self.conv_blocks:\n",
    "            # x, idx = conv_block(x)\n",
    "            # indices.append(idx)\n",
    "        # return self.flatten(x), indices\n",
    "\n",
    "\n",
    "class UnConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            stride (int): Controls the stride.\n",
    "        \"\"\"\n",
    "        super(UnConvBlock, self).__init__()\n",
    "\n",
    "        # self.unpool = nn.MaxUnpool2d(kernel_size=3, stride=2)\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(in_channels).to(device),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, in_channels, kernel_size=3, padding=1, stride=1\n",
    "            ).to(device),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(in_channels).to(device),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size=3, padding=1, stride=1\n",
    "            ).to(device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.unpool(x, indices, output_size=shape)\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "    # def forward(self, x, indices, shape):\n",
    "    #     x = self.unpool(x, indices, output_size=shape)\n",
    "    #     x = self.block(x)\n",
    "    #     return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_hidden, n_cats, n_latent, n_square, device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear = nn.Linear(n_hidden + n_cats, n_latent).to(device)\n",
    "        self.unflatten = nn.Unflatten(1, [64, n_square, n_square])\n",
    "        self.conv_blocks = (\n",
    "            # UnConvBlock(256, 128, device),\n",
    "            # UnConvBlock(128, 64, device),\n",
    "            UnConvBlock(64, 32, device),\n",
    "            UnConvBlock(32, 16, device),\n",
    "            UnConvBlock(16, 8, device),\n",
    "            UnConvBlock(8, 3, device),\n",
    "        )\n",
    "        # self.shapes = (\n",
    "        #     (1, 512, 7, 7),\n",
    "        #     (1, 256, 15, 15),\n",
    "        #     (1, 128, 31, 31),\n",
    "        #     (1, 64, 64, 64),\n",
    "        #     (1, 64, 64, 64),\n",
    "        # )\n",
    "        # self.lin_block = nn.Sequential(\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(3 * 64 * 64, 3 * 64 * 64),\n",
    "        #     nn.Unflatten(1, (3, 64, 64)),\n",
    "        # ).to(device)\n",
    "        self.softmax = nn.Softmax(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.unflatten(self.linear(x))\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "        # x = self.lin_block(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    # def forward(self, x, indices):\n",
    "    #     x = self.unflatten(self.linear(x))\n",
    "    #     for conv_block, idx, shape in zip(self.conv_blocks, reversed(indices), self.shapes):\n",
    "    #         x = conv_block(x, idx, shape)\n",
    "    #     x = self.lin_block(x)\n",
    "    #     return x\n",
    "\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        n_hidden=400,\n",
    "        n_cats=32,\n",
    "        n_ff_hidden=1024,\n",
    "        dropout=0.5,\n",
    "        mix=0.8,\n",
    "        kl_weight=0.5,\n",
    "    ):\n",
    "        super(WorldModel, self).__init__()\n",
    "        self.dev = device\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_cats = n_cats\n",
    "        self.n_ff_hidden = n_ff_hidden\n",
    "        self.dropout = dropout\n",
    "        self.mix = mix\n",
    "        self.kl_weight = kl_weight\n",
    "        self.resize = Resize((64, 64))\n",
    "        self.encoder_model = Encoder(device).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test = torch.zeros((1, 64, 64, 3))\n",
    "            self.n_latent = self.encoder_model(self.preprocess(test)).shape[1]\n",
    "            # self.n_latent = self.encoder_model(self.preprocess(test))[0].shape[1]\n",
    "            self.n_square = int(np.sqrt(self.n_latent / 64))\n",
    "\n",
    "        self.decoder_model = Decoder(n_hidden, n_cats, self.n_latent, self.n_square, device).to(device)\n",
    "        self.rssm = nn.GRUCell(n_cats + 1, n_hidden).to(device)\n",
    "        self.posterior_model = nn.Sequential(\n",
    "            nn.Linear(self.n_latent + n_hidden, n_ff_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_ff_hidden, n_cats * n_cats),\n",
    "            nn.Unflatten(1, (n_cats, n_cats)),\n",
    "            nn.Softmax(2),\n",
    "        ).to(device)\n",
    "        self.prior_model = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_ff_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_ff_hidden, n_cats * n_cats),\n",
    "            nn.Unflatten(1, (n_cats, n_cats)),\n",
    "            nn.Softmax(2),\n",
    "        ).to(device)\n",
    "        self.reward_model = nn.Sequential(\n",
    "            nn.Linear(n_hidden + n_cats, n_ff_hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_ff_hidden, 1),\n",
    "        ).to(device)\n",
    "        self.discount_model = nn.Sequential(\n",
    "            nn.Linear(n_hidden + n_cats, n_ff_hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_ff_hidden, 1),\n",
    "            nn.Softmax(1),\n",
    "        ).to(device)\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        img = torch.tensor(img).to(self.dev).permute([0, 3, 1, 2])\n",
    "        return self.resize(img) / 255.0\n",
    "\n",
    "    def get_real_states(self, img, hidden):\n",
    "        prior = self.prior_model(hidden)\n",
    "\n",
    "        std_img = self.preprocess(img)\n",
    "        encoded_img = self.encoder_model(std_img)\n",
    "        # encoded_img, indices = self.encoder_model(std_img)\n",
    "        posterior = self.posterior_model(torch.cat((encoded_img, hidden), 1))\n",
    "\n",
    "        posterior_dist = Categorical(posterior)\n",
    "        posterior_sample = posterior_dist.sample().float()\n",
    "\n",
    "        state = torch.cat((hidden, posterior_sample), 1)\n",
    "\n",
    "        gen_img = self.decoder_model(state)\n",
    "        # gen_img = self.decoder_model(state, indices)\n",
    "        reward = self.reward_model(state)\n",
    "        discount = self.discount_model(state)\n",
    "\n",
    "        return prior, posterior, gen_img, std_img, reward, discount\n",
    "\n",
    "    def get_next_hidden(self, hidden, dist, action):\n",
    "        sample = Categorical(dist).sample()\n",
    "        action = torch.tensor([[action]]).float().to(self.dev)\n",
    "        return self.rssm(torch.cat((sample, action), 1), hidden)\n",
    "\n",
    "    def get_dream_states(self, hidden, img=None):\n",
    "        if img is None:\n",
    "            posterior = self.prior_model(hidden)\n",
    "        else:\n",
    "            std_img = self.preprocess(img)\n",
    "            encoded_img = self.encoder_model(std_img)\n",
    "            # encoded_img, _ = self.encoder_model(std_img)\n",
    "            posterior = self.posterior_model(torch.cat((encoded_img, hidden)))\n",
    "\n",
    "        posterior_dist = Categorical(posterior)\n",
    "        posterior_sample = posterior_dist.sample()\n",
    "\n",
    "        state = torch.cat((hidden, posterior_sample), 1)\n",
    "\n",
    "        reward = self.reward_model(state)\n",
    "        discount = self.discount_model(state)\n",
    "\n",
    "        return state, reward, discount\n",
    "\n",
    "    def calc_loss(\n",
    "        self,\n",
    "        prior,\n",
    "        posterior,\n",
    "        gen_img,\n",
    "        true_img,\n",
    "        gen_reward,\n",
    "        true_reward,\n",
    "        gen_discount,\n",
    "        true_discount,\n",
    "    ):\n",
    "        img_loss = F.binary_cross_entropy(gen_img, true_img)\n",
    "        true_reward = torch.tensor([[true_reward]]).to(self.dev)\n",
    "        true_discount = torch.tensor([[true_discount]]).to(self.dev)\n",
    "        reward_loss = F.mse_loss(gen_reward, true_reward)\n",
    "        discount_loss = F.binary_cross_entropy(gen_discount, true_discount)\n",
    "        prior1 = Categorical(prior)\n",
    "        prior2 = Categorical(prior.detach())\n",
    "        posterior1 = Categorical(posterior.detach())\n",
    "        posterior2 = Categorical(posterior)\n",
    "        kl_div_loss = self.mix * kl_divergence(posterior1, prior1).mean() + (\n",
    "            1 - self.mix\n",
    "        ) * kl_divergence(posterior2, prior2).mean()\n",
    "        return (\n",
    "            img_loss + 0.2 * reward_loss + 0.1 * discount_loss + self.kl_weight * kl_div_loss,\n",
    "            np.array((img_loss.detach().cpu().numpy(), reward_loss.detach().cpu().numpy(), discount_loss.detach().cpu().numpy(), kl_div_loss.detach().cpu().numpy(),)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6675235,
     "status": "ok",
     "timestamp": 1626748633306,
     "user": {
      "displayName": "Tony Schroeder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilICEJmMvke-YcAXapHV2CDnsfjBfDiGICD336=s64",
      "userId": "03301841124230681313"
     },
     "user_tz": 420
    },
    "id": "bpDL3quVBUw9",
    "outputId": "83899844-52ad-4fd9-f45d-cda8b14fa123",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:196: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Total Avg Loss: 5.907382582902908, img_loss: 1.4584908065795898, reward_loss: 19.488701261712222, discount_loss: 0.09999871253967285, kl_div: 1.082303167335689\n",
      "Step 1000: Total Avg Loss: 2.1006303572654725, img_loss: 1.4293652546405793, reward_loss: 3.2326896075644806, discount_loss: 0.09999871253967285, kl_div: 0.02945464228815399\n",
      "Finished with score: -20.0\n",
      "Step 1500: Total Avg Loss: 1.9695443489551545, img_loss: 1.4269009804725648, reward_loss: 2.4527328613448716, discount_loss: 0.2997987151145935, kl_div: 0.04423385730118025\n",
      "Step 2000: Total Avg Loss: 1.6541774096488953, img_loss: 1.4220073504447936, reward_loss: 1.058789432074741, discount_loss: 0.09999871253967285, kl_div: 0.0208246129869367\n",
      "Finished with score: -21.0\n",
      "Step 2500: Total Avg Loss: 1.6675036211013794, img_loss: 1.4214271953105926, reward_loss: 1.061969934187051, discount_loss: 0.2997987151145935, kl_div: 0.0074051399477466475\n",
      "Step 3000: Total Avg Loss: 1.529678537607193, img_loss: 1.4206403663158418, reward_loss: 0.4923977718735227, discount_loss: 0.09999871253967285, kl_div: 0.0011174834637276945\n",
      "Step 3500: Total Avg Loss: 1.5609860515594483, img_loss: 1.4206368546485901, reward_loss: 0.6432472617629683, discount_loss: 0.09999871253967285, kl_div: 0.0033997408979121245\n",
      "Finished with score: -21.0\n",
      "Step 4000: Total Avg Loss: 1.593912444114685, img_loss: 1.4199295687675475, reward_loss: 0.7119728759483406, discount_loss: 0.2997987151145935, kl_div: 0.003216858379120822\n",
      "Step 4500: Total Avg Loss: 1.5350893683433533, img_loss: 1.4201224501132965, reward_loss: 0.5208943148944434, discount_loss: 0.09999871253967285, kl_div: 0.0015763650309163495\n",
      "Finished with score: -21.0\n",
      "Step 5000: Total Avg Loss: 1.54583349442482, img_loss: 1.4194538300037385, reward_loss: 0.471143779712649, discount_loss: 0.2997987151145935, kl_div: 0.004342066959106887\n",
      "Step 5500: Total Avg Loss: 1.5640794396400453, img_loss: 1.419760219812393, reward_loss: 0.6515454462347207, discount_loss: 0.09999871253967285, kl_div: 0.008020506380358712\n",
      "Step 6000: Total Avg Loss: 1.5594543087482453, img_loss: 1.419475153207779, reward_loss: 0.6402469585368226, discount_loss: 0.09999871253967285, kl_div: 0.003859786559762142\n",
      "Finished with score: -21.0\n",
      "Step 6500: Total Avg Loss: 1.5298037519454957, img_loss: 1.419707524061203, reward_loss: 0.38958960944285337, discount_loss: 0.2997987151145935, kl_div: 0.004396867200732231\n",
      "Step 7000: Total Avg Loss: 1.5077780089378356, img_loss: 1.4195603415966034, reward_loss: 0.38233383772668594, discount_loss: 0.09999871253967285, kl_div: 0.0035020490652259467\n",
      "Finished with score: -21.0\n",
      "Step 7500: Total Avg Loss: 1.5793790516853332, img_loss: 1.419270043849945, reward_loss: 0.6466172877869704, discount_loss: 0.2997987151145935, kl_div: 0.001611359427575735\n",
      "Step 8000: Total Avg Loss: 1.5523774144649505, img_loss: 1.4195141062736512, reward_loss: 0.6116891437621131, discount_loss: 0.09999871253967285, kl_div: 0.0010512123219978094\n",
      "Finished with score: -20.0\n",
      "Step 8500: Total Avg Loss: 1.5170846605300903, img_loss: 1.419551290988922, reward_loss: 0.3355223384132398, discount_loss: 0.2997987151145935, kl_div: 0.0008980604016528559\n",
      "Step 9000: Total Avg Loss: 1.498311495065689, img_loss: 1.4193040776252746, reward_loss: 0.344997532284161, discount_loss: 0.09999871253967285, kl_div: 1.6074347273615784e-05\n",
      "Finished with score: -21.0\n",
      "Step 9500: Total Avg Loss: 1.5633293075561523, img_loss: 1.4190780155658722, reward_loss: 0.5663461982615227, discount_loss: 0.2997987151145935, kl_div: 0.0020043694563692043\n",
      "Step 10000: Total Avg Loss: 1.5302959418296813, img_loss: 1.4194417853355408, reward_loss: 0.5042512691867596, discount_loss: 0.09999871253967285, kl_div: 8.064098129466402e-06\n",
      "Step 10500: Total Avg Loss: 1.5455785419940948, img_loss: 1.4191379470825196, reward_loss: 0.5796992408616959, discount_loss: 0.09999871253967285, kl_div: 0.0010017576139738367\n",
      "Finished with score: -21.0\n",
      "Step 11000: Total Avg Loss: 1.5167111279964447, img_loss: 1.4190742695331573, reward_loss: 0.3382746082504127, discount_loss: 0.2997987151145935, kl_div: 4.1300500378156355e-06\n",
      "Step 11500: Total Avg Loss: 1.5161656322479249, img_loss: 1.419069224357605, reward_loss: 0.43547579189581337, discount_loss: 0.09999871253967285, kl_div: 2.749816535555283e-06\n",
      "Step 12000: Total Avg Loss: 1.5439843063354493, img_loss: 1.4195694925785065, reward_loss: 0.5695777574789336, discount_loss: 0.09999871253967285, kl_div: 0.0009987773255560057\n",
      "Finished with score: -20.0\n",
      "Step 12500: Total Avg Loss: 1.5750277740955352, img_loss: 1.4190521993637084, reward_loss: 0.6299746538837446, discount_loss: 0.2997987151145935, kl_div: 1.5413641331960549e-06\n",
      "Step 13000: Total Avg Loss: 1.5222431478500367, img_loss: 1.4191965613365174, reward_loss: 0.4652306398155499, discount_loss: 0.09999871253967285, kl_div: 1.1692603333521844e-06\n",
      "Finished with score: -20.0\n",
      "Step 13500: Total Avg Loss: 1.5190431485176086, img_loss: 1.4191445889472962, reward_loss: 0.3473986836134586, discount_loss: 0.2997987151145935, kl_div: 0.000877897614900327\n",
      "Step 14000: Total Avg Loss: 1.5237242164611817, img_loss: 1.4189835970401763, reward_loss: 0.47369843414341994, discount_loss: 0.09999871253967285, kl_div: 2.1244955201353834e-06\n",
      "Step 14500: Total Avg Loss: 1.512930554151535, img_loss: 1.4191682906150818, reward_loss: 0.41880933588643166, discount_loss: 0.09999871253967285, kl_div: 1.0467103167925984e-06\n",
      "Finished with score: -19.0\n",
      "Step 15000: Total Avg Loss: 1.5609500467777253, img_loss: 1.419143001317978, reward_loss: 0.5566433355161887, discount_loss: 0.2997987151145935, kl_div: 0.0009970194222136746\n",
      "Step 15500: Total Avg Loss: 1.5180672578811645, img_loss: 1.419022495985031, reward_loss: 0.44522331095589107, discount_loss: 0.09999871253967285, kl_div: 4.587780483884529e-07\n",
      "Finished with score: -21.0\n",
      "Step 16000: Total Avg Loss: 1.512511696100235, img_loss: 1.4189955053329468, reward_loss: 0.31768073483609166, discount_loss: 0.2997987151145935, kl_div: 3.538617440437264e-07\n",
      "Step 16500: Total Avg Loss: 1.5059908332824707, img_loss: 1.4189904375076294, reward_loss: 0.3850021925017186, discount_loss: 0.09999871253967285, kl_div: 1.9408121655395406e-07\n",
      "Step 17000: Total Avg Loss: 1.5482293603420259, img_loss: 1.4192484958171845, reward_loss: 0.5921966576429404, discount_loss: 0.09999871253967285, kl_div: 0.0010833430150547657\n",
      "Finished with score: -20.0\n",
      "Step 17500: Total Avg Loss: 1.5500842199325562, img_loss: 1.419145307302475, reward_loss: 0.5047949457576683, discount_loss: 0.2997987151145935, kl_div: 1.2691761203420528e-07\n",
      "Step 18000: Total Avg Loss: 1.552158063173294, img_loss: 1.4190314540863036, reward_loss: 0.6082465860519373, discount_loss: 0.09999871253967285, kl_div: 0.0029548521733809333\n",
      "Step 18500: Total Avg Loss: 1.514368108034134, img_loss: 1.4190244417190552, reward_loss: 0.42671847497975773, discount_loss: 0.09999871253967285, kl_div: 2.1077083723408663e-07\n",
      "Finished with score: -19.0\n",
      "Step 19000: Total Avg Loss: 1.5428155298233033, img_loss: 1.4189930138587952, reward_loss: 0.4692130063335743, discount_loss: 0.2997987151145935, kl_div: 1.0314987548287746e-07\n",
      "Step 19500: Total Avg Loss: 1.5356109826564788, img_loss: 1.4190839560031892, reward_loss: 0.5326355953763864, discount_loss: 0.09999871253967285, kl_div: 8.298111543236786e-08\n",
      "Finished with score: -21.0\n",
      "Step 20000: Total Avg Loss: 1.5407572588920593, img_loss: 1.4194867672920226, reward_loss: 0.45645296947451336, discount_loss: 0.2997987151145935, kl_div: 7.040066542707279e-08\n",
      "Step 20500: Total Avg Loss: 1.4858204369544983, img_loss: 1.418937207698822, reward_loss: 0.28393862524008184, discount_loss: 0.09999871253967285, kl_div: 0.00019127765302762167\n",
      "Finished with score: -20.0\n",
      "Step 21000: Total Avg Loss: 1.5698331642150878, img_loss: 1.41901100897789, reward_loss: 0.6017189629901841, discount_loss: 0.2997987151145935, kl_div: 0.0009969893836994732\n",
      "Step 21500: Total Avg Loss: 1.5256781117916107, img_loss: 1.4190261073112487, reward_loss: 0.48326062723659324, discount_loss: 0.09999871253967285, kl_div: 1.9997384036685163e-08\n",
      "Step 22000: Total Avg Loss: 1.4932296538352967, img_loss: 1.4189910733699798, reward_loss: 0.3211935262722163, discount_loss: 0.09999871253967285, kl_div: 1.2431307576221684e-08\n",
      "Finished with score: -21.0\n",
      "Step 22500: Total Avg Loss: 1.54908141207695, img_loss: 1.4189891662597656, reward_loss: 0.5005618672562072, discount_loss: 0.2997987151145935, kl_div: 1.0777290448515942e-08\n",
      "Step 23000: Total Avg Loss: 1.520155833005905, img_loss: 1.4189046065807343, reward_loss: 0.45376575842842015, discount_loss: 0.09999871253967285, kl_div: 0.0009964061677517017\n",
      "Finished with score: -19.0\n",
      "Step 23500: Total Avg Loss: 1.5590209238529205, img_loss: 1.4191261954307557, reward_loss: 0.5470832625634422, discount_loss: 0.2997987151145935, kl_div: 0.0009964045174477114\n",
      "Step 24000: Total Avg Loss: 1.5226680812835693, img_loss: 1.4191691970825195, reward_loss: 0.4625130651861239, discount_loss: 0.09999871253967285, kl_div: 0.001992802895617051\n",
      "Step 24500: Total Avg Loss: 1.5293000936508179, img_loss: 1.4188759903907775, reward_loss: 0.4996301606817926, discount_loss: 0.09999871253967285, kl_div: 0.0009964017979822613\n",
      "Finished with score: -19.0\n",
      "Step 25000: Total Avg Loss: 1.5845317213535308, img_loss: 1.4191835572719573, reward_loss: 0.6743504581833654, discount_loss: 0.2997987151145935, kl_div: 0.0009964009262638828\n",
      "Step 25500: Total Avg Loss: 1.5441906900405884, img_loss: 1.4189419043064118, reward_loss: 0.573753057579399, discount_loss: 0.09999871253967285, kl_div: 0.0009966025602197383\n",
      "Finished with score: -21.0\n",
      "Step 26000: Total Avg Loss: 1.5483104610443115, img_loss: 1.4191324474811553, reward_loss: 0.4934997167047122, discount_loss: 0.2997987151145935, kl_div: 0.0009964006021630496\n",
      "Step 26500: Total Avg Loss: 1.5365478019714356, img_loss: 1.4189959974288942, reward_loss: 0.5377596599124208, discount_loss: 0.09999871253967285, kl_div: 1.24052243233308e-09\n",
      "Finished with score: -20.0\n",
      "Step 27000: Total Avg Loss: 1.5317427868843079, img_loss: 1.4189052937030793, reward_loss: 0.4142881203123197, discount_loss: 0.2997987151145935, kl_div: 1.0654336319859682e-09\n",
      "Step 27500: Total Avg Loss: 1.4967292356491089, img_loss: 1.4188928866386414, reward_loss: 0.33918237857332695, discount_loss: 0.09999871253967285, kl_div: 8.38191231977703e-10\n",
      "Step 28000: Total Avg Loss: 1.5152800266742705, img_loss: 1.4189792313575744, reward_loss: 0.4315046232588002, discount_loss: 0.09999871253967285, kl_div: 8.083893714427859e-10\n",
      "Finished with score: -19.0\n",
      "Step 28500: Total Avg Loss: 1.5350530717372894, img_loss: 1.4189636034965516, reward_loss: 0.43054797710729964, discount_loss: 0.2997987151145935, kl_div: 7.78586286109828e-10\n",
      "Step 29000: Total Avg Loss: 1.5329560377597808, img_loss: 1.4189522037506104, reward_loss: 0.5200198036181283, discount_loss: 0.09999871253967285, kl_div: 2.5704510253632404e-10\n",
      "Step 29500: Total Avg Loss: 1.5597319395542144, img_loss: 1.4188937590122224, reward_loss: 0.651700539708526, discount_loss: 0.09999871253967285, kl_div: 0.000996399726719007\n",
      "Finished with score: -20.0\n",
      "Step 30000: Total Avg Loss: 1.5869671721458436, img_loss: 1.4192465937137604, reward_loss: 0.6837215853890222, discount_loss: 0.2997987151145935, kl_div: 0.0019927762784064606\n",
      "Step 30500: Total Avg Loss: 1.5091973843574524, img_loss: 1.4188604800701141, reward_loss: 0.40168515448475967, discount_loss: 0.09999871253967285, kl_div: 2.9057273653876335e-10\n",
      "Finished with score: -21.0\n",
      "Step 31000: Total Avg Loss: 1.5181188559532166, img_loss: 1.4190523931980132, reward_loss: 0.3454329556171208, discount_loss: 0.2997987151145935, kl_div: 1.9744048884717813e-10\n",
      "Step 31500: Total Avg Loss: 1.5208933477401734, img_loss: 1.4189229166507722, reward_loss: 0.4573617963179795, discount_loss: 0.09999871253967285, kl_div: 0.000996399156749285\n",
      "Finished with score: -21.0\n",
      "Step 32000: Total Avg Loss: 1.549004200220108, img_loss: 1.4188510749340058, reward_loss: 0.500866275505792, discount_loss: 0.2997987151145935, kl_div: 1.2076526960669298e-09\n",
      "Step 32500: Total Avg Loss: 1.4911391091346742, img_loss: 1.4190063190460205, reward_loss: 0.31007207413546234, discount_loss: 0.09999871253967285, kl_div: 0.0002370063923299721\n",
      "Step 33000: Total Avg Loss: 1.5312402505874634, img_loss: 1.4189458355903626, reward_loss: 0.5114726973251654, discount_loss: 0.09999871253967285, kl_div: 3.5017745236842757e-10\n",
      "Finished with score: -19.0\n",
      "Step 33500: Total Avg Loss: 1.5454260013103485, img_loss: 1.419132337808609, reward_loss: 0.48156896274528266, discount_loss: 0.2997987151145935, kl_div: 1.713634008027043e-10\n",
      "Step 34000: Total Avg Loss: 1.490075353860855, img_loss: 1.4189015710353852, reward_loss: 0.30586955777158076, discount_loss: 0.09999871253967285, kl_div: 2.682211990290284e-10\n",
      "Step 34500: Total Avg Loss: 1.5225995147228242, img_loss: 1.4188566222190857, reward_loss: 0.4662240986163215, discount_loss: 0.09999871253967285, kl_div: 0.0009963991492987008\n",
      "Finished with score: -20.0\n",
      "Step 35000: Total Avg Loss: 1.5611330943107604, img_loss: 1.418970553636551, reward_loss: 0.560913323820848, discount_loss: 0.2997987151145935, kl_div: 1.4528637581889824e-10\n",
      "Step 35500: Total Avg Loss: 1.543644324541092, img_loss: 1.4188934378623963, reward_loss: 0.5737550931617105, discount_loss: 0.09999871253967285, kl_div: 5.960465365717482e-11\n",
      "Finished with score: -21.0\n",
      "Step 36000: Total Avg Loss: 1.6453801789283753, img_loss: 1.419253725528717, reward_loss: 0.9583140040144672, discount_loss: 0.2997987151145935, kl_div: 0.008967567857354926\n",
      "Step 36500: Total Avg Loss: 1.5749796030521392, img_loss: 1.418900963306427, reward_loss: 0.7279028460587049, discount_loss: 0.09999871253967285, kl_div: 0.0009963991530240071\n",
      "Finished with score: -21.0\n",
      "Step 37000: Total Avg Loss: 1.5390109481811522, img_loss: 1.4190222880840302, reward_loss: 0.44958383402958707, discount_loss: 0.2997987151145935, kl_div: 0.00018404454737906662\n",
      "Step 37500: Total Avg Loss: 1.499875096321106, img_loss: 1.418878000974655, reward_loss: 0.3549861281962503, discount_loss: 0.09999871253967285, kl_div: 8.940698936754643e-11\n",
      "Step 38000: Total Avg Loss: 1.501611221075058, img_loss: 1.4189263236522676, reward_loss: 0.363425114588002, discount_loss: 0.09999871253967285, kl_div: 9.685757529354077e-11\n",
      "Finished with score: -20.0\n",
      "Step 38500: Total Avg Loss: 1.529427605867386, img_loss: 1.4188463306427002, reward_loss: 0.4030070017644623, discount_loss: 0.2997987151145935, kl_div: 2.6077039194660755e-11\n",
      "Step 39000: Total Avg Loss: 1.51891592669487, img_loss: 1.4188806302547454, reward_loss: 0.450177118232637, discount_loss: 0.09999871253967285, kl_div: 8.940698581483275e-11\n",
      "Finished with score: -21.0\n",
      "Step 39500: Total Avg Loss: 1.5652627065181732, img_loss: 1.419070621728897, reward_loss: 0.5785700882339622, discount_loss: 0.2997987151145935, kl_div: 0.0009963991828263667\n",
      "Step 40000: Total Avg Loss: 1.5367780611515045, img_loss: 1.4188834948539735, reward_loss: 0.5394734603345935, discount_loss: 0.09999871253967285, kl_div: 1.1920934372966484e-10\n",
      "Finished with score: -21.0\n",
      "Step 40500: Total Avg Loss: 1.513135382413864, img_loss: 1.4188571469783784, reward_loss: 0.3214918321120067, discount_loss: 0.2997987151145935, kl_div: 1.4901162970204496e-11\n",
      "Step 41000: Total Avg Loss: 1.54430024600029, img_loss: 1.4190804600715636, reward_loss: 0.5760995818304925, discount_loss: 0.09999871253967285, kl_div: 1.1175874359281579e-10\n",
      "Step 41500: Total Avg Loss: 1.5565240190029144, img_loss: 1.4188418028354646, reward_loss: 0.6359207333745552, discount_loss: 0.09999871253967285, kl_div: 0.0009963991269469562\n",
      "Finished with score: -20.0\n",
      "Step 42000: Total Avg Loss: 1.564954422235489, img_loss: 1.4189710915088654, reward_loss: 0.5775263034859625, discount_loss: 0.2997987151145935, kl_div: 0.0009963990822434469\n",
      "Step 42500: Total Avg Loss: 1.505603426218033, img_loss: 1.4188634521961212, reward_loss: 0.38370051683033046, discount_loss: 0.09999871253967285, kl_div: 1.1175873559921002e-11\n",
      "Finished with score: -20.0\n",
      "Step 43000: Total Avg Loss: 1.5247572276592254, img_loss: 1.4188486433029175, reward_loss: 0.3796435682416568, discount_loss: 0.2997987151145935, kl_div: 1.490116634528249e-10\n",
      "Step 43500: Total Avg Loss: 1.4966652338504791, img_loss: 1.4189256591796875, reward_loss: 0.33869851481822527, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 44000: Total Avg Loss: 1.5214611690044404, img_loss: 1.4188956334590912, reward_loss: 0.3629283006197557, discount_loss: 0.2997987151145935, kl_div: 1.1175871783564162e-11\n",
      "Step 44500: Total Avg Loss: 1.5089348728656768, img_loss: 1.4188912818431854, reward_loss: 0.4002186071584094, discount_loss: 0.09999871253967285, kl_div: 1.1175870895385742e-11\n",
      "Step 45000: Total Avg Loss: 1.5246238763332367, img_loss: 1.4188570413589479, reward_loss: 0.47883481934053634, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 45500: Total Avg Loss: 1.5553576991558076, img_loss: 1.4188480153083802, reward_loss: 0.5326490533133983, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 46000: Total Avg Loss: 1.5152287561893463, img_loss: 1.4188973343372344, reward_loss: 0.43165776339406875, discount_loss: 0.09999871253967285, kl_div: 9.313232940399984e-11\n",
      "Finished with score: -20.0\n",
      "Step 46500: Total Avg Loss: 1.538204151391983, img_loss: 1.418932920217514, reward_loss: 0.4464567792236079, discount_loss: 0.2997987151145935, kl_div: 1.1175872671742582e-11\n",
      "Step 47000: Total Avg Loss: 1.526461003780365, img_loss: 1.4188502321243286, reward_loss: 0.48805450658263, discount_loss: 0.09999871253967285, kl_div: 7.450581485102248e-12\n",
      "Step 47500: Total Avg Loss: 1.4887030792236329, img_loss: 1.4189110367298126, reward_loss: 0.2989608486355919, discount_loss: 0.09999871253967285, kl_div: 7.450581485102248e-12\n",
      "Finished with score: -19.0\n",
      "Step 48000: Total Avg Loss: 1.519807230234146, img_loss: 1.418826490163803, reward_loss: 0.3550043251859268, discount_loss: 0.2997987151145935, kl_div: 7.450581485102248e-12\n",
      "Step 48500: Total Avg Loss: 1.5163578007221221, img_loss: 1.4188794224262238, reward_loss: 0.4373925437678754, discount_loss: 0.09999871253967285, kl_div: 7.450581485102248e-12\n",
      "Finished with score: -20.0\n",
      "Step 49000: Total Avg Loss: 1.5458624455928802, img_loss: 1.4188504025936126, reward_loss: 0.48516085447925705, discount_loss: 0.2997987151145935, kl_div: 4.8428785426324336e-11\n",
      "Step 49500: Total Avg Loss: 1.4952196514606475, img_loss: 1.4188545808792115, reward_loss: 0.3318259832096583, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Step 50000: Total Avg Loss: 1.5233988409042358, img_loss: 1.418910837173462, reward_loss: 0.47244065315812894, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 50500: Total Avg Loss: 1.686523529291153, img_loss: 1.4191290798187255, reward_loss: 1.1845818928297722, discount_loss: 0.2997987151145935, kl_div: 0.0009963990636169928\n",
      "Step 51000: Total Avg Loss: 1.5034840986728668, img_loss: 1.4188348436355591, reward_loss: 0.3732469189504565, discount_loss: 0.09999871253967285, kl_div: 1.4901163858382916e-11\n",
      "Finished with score: -20.0\n",
      "Step 51500: Total Avg Loss: 1.5150377805233002, img_loss: 1.4188381893634796, reward_loss: 0.3310986012349322, discount_loss: 0.2997987151145935, kl_div: 1.4901164746561336e-11\n",
      "Step 52000: Total Avg Loss: 1.4753879141807555, img_loss: 1.4189506349563599, reward_loss: 0.2321870549686652, discount_loss: 0.09999871253967285, kl_div: 1.4901162970204496e-11\n",
      "Step 52500: Total Avg Loss: 1.490553884267807, img_loss: 1.4188281092643737, reward_loss: 0.30862951219622664, discount_loss: 0.09999871253967285, kl_div: 3.725290298461914e-12\n",
      "Finished with score: -20.0\n",
      "Step 53000: Total Avg Loss: 1.5069176516532898, img_loss: 1.418902158021927, reward_loss: 0.29017810813906386, discount_loss: 0.2997987151145935, kl_div: 1.4901164746561336e-11\n",
      "Step 53500: Total Avg Loss: 1.51553031539917, img_loss: 1.4188682761192322, reward_loss: 0.43331083947260596, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 54000: Total Avg Loss: 1.5206423666477202, img_loss: 1.4188183779716492, reward_loss: 0.3592205840857995, discount_loss: 0.2997987151145935, kl_div: 2.607703564194708e-11\n",
      "Step 54500: Total Avg Loss: 1.5215584177970887, img_loss: 1.418932484149933, reward_loss: 0.4631303125612667, discount_loss: 0.09999871253967285, kl_div: 3.725290298461914e-12\n",
      "Finished with score: -21.0\n",
      "Step 55000: Total Avg Loss: 1.5418982253074647, img_loss: 1.418850132226944, reward_loss: 0.46534111397317246, discount_loss: 0.2997987151145935, kl_div: 3.725290298461914e-12\n",
      "Step 55500: Total Avg Loss: 1.5116547539234162, img_loss: 1.4188278994560242, reward_loss: 0.41413492185446693, discount_loss: 0.09999871253967285, kl_div: 4.842878276178908e-11\n",
      "Step 56000: Total Avg Loss: 1.5099867680072785, img_loss: 1.4188828818798065, reward_loss: 0.4055200616980539, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -20.0\n",
      "Step 56500: Total Avg Loss: 1.5185087797641754, img_loss: 1.4188905086517334, reward_loss: 0.3481919910888745, discount_loss: 0.2997987151145935, kl_div: 3.725291719547385e-11\n",
      "Step 57000: Total Avg Loss: 1.4929437704086304, img_loss: 1.4188250179290771, reward_loss: 0.32059439607495127, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Step 57500: Total Avg Loss: 1.5161426904201507, img_loss: 1.4188153519630433, reward_loss: 0.43663734851960273, discount_loss: 0.09999871253967285, kl_div: 3.7252914530938597e-11\n",
      "Finished with score: -20.0\n",
      "Step 58000: Total Avg Loss: 1.5288744325637817, img_loss: 1.4188912529945374, reward_loss: 0.40001653309761787, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 58500: Total Avg Loss: 1.5318809235095978, img_loss: 1.4188819460868836, reward_loss: 0.5149955021002824, discount_loss: 0.09999871253967285, kl_div: 7.450581485102248e-12\n",
      "Step 59000: Total Avg Loss: 1.518876897096634, img_loss: 1.4188279449939727, reward_loss: 0.45024539735942776, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -18.0\n",
      "Step 59500: Total Avg Loss: 1.5467330462932587, img_loss: 1.4189572336673737, reward_loss: 0.48897970189204126, discount_loss: 0.2997987151145935, kl_div: 2.2351746231663584e-11\n",
      "Step 60000: Total Avg Loss: 1.5276781194210052, img_loss: 1.4189586367607117, reward_loss: 0.4935980581373667, discount_loss: 0.09999871253967285, kl_div: 7.450581485102248e-12\n",
      "Finished with score: -20.0\n",
      "Step 60500: Total Avg Loss: 1.5410005643367768, img_loss: 1.4188283176422118, reward_loss: 0.46096186364750863, discount_loss: 0.2997987151145935, kl_div: 3.725290298461914e-12\n",
      "Step 61000: Total Avg Loss: 1.5180578486919403, img_loss: 1.4188269329071046, reward_loss: 0.4461552141564971, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Step 61500: Total Avg Loss: 1.514147334575653, img_loss: 1.4188847544193268, reward_loss: 0.4263135544686963, discount_loss: 0.09999871253967285, kl_div: 1.4901162970204496e-11\n",
      "Finished with score: -20.0\n",
      "Step 62000: Total Avg Loss: 1.515713561296463, img_loss: 1.4188552553653717, reward_loss: 0.3343921579280451, discount_loss: 0.2997987151145935, kl_div: 7.450581485102248e-12\n",
      "Step 62500: Total Avg Loss: 1.5027064566612243, img_loss: 1.4188885085582734, reward_loss: 0.36909037815639767, discount_loss: 0.09999871253967285, kl_div: 2.235174889619884e-11\n",
      "Finished with score: -21.0\n",
      "Step 63000: Total Avg Loss: 1.5208545651435852, img_loss: 1.4188208169937133, reward_loss: 0.3602693798258042, discount_loss: 0.2997987151145935, kl_div: 7.450581485102248e-12\n",
      "Step 63500: Total Avg Loss: 1.5054409127235413, img_loss: 1.418842230796814, reward_loss: 0.38050307370666997, discount_loss: 0.09999871253967285, kl_div: 0.0009963990524411211\n",
      "Finished with score: -21.0\n",
      "Step 64000: Total Avg Loss: 1.5889184036254882, img_loss: 1.4189667730331421, reward_loss: 0.6998587904067656, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 64500: Total Avg Loss: 1.5560257713794707, img_loss: 1.4188476083278656, reward_loss: 0.6358914505181906, discount_loss: 0.09999871253967285, kl_div: 1.1175871783564162e-11\n",
      "Step 65000: Total Avg Loss: 1.5136332778930663, img_loss: 1.4189711272716523, reward_loss: 0.4208203953159447, discount_loss: 0.09999871253967285, kl_div: 0.0009963990487158298\n",
      "Finished with score: -17.0\n",
      "Step 65500: Total Avg Loss: 1.5284449186325073, img_loss: 1.4188273029327392, reward_loss: 0.3981887094195172, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 66000: Total Avg Loss: 1.5394322988986968, img_loss: 1.4188135135173798, reward_loss: 0.5506035671970663, discount_loss: 0.09999871253967285, kl_div: 0.0009963991940023361\n",
      "Finished with score: -21.0\n",
      "Step 66500: Total Avg Loss: 1.559703262090683, img_loss: 1.4188670699596404, reward_loss: 0.5542816024825006, discount_loss: 0.2997987151145935, kl_div: 7.450581485102248e-12\n",
      "Step 67000: Total Avg Loss: 1.482649671792984, img_loss: 1.418842396259308, reward_loss: 0.2690370165145614, discount_loss: 0.09999871253967285, kl_div: 1.862645504502325e-11\n",
      "Step 67500: Total Avg Loss: 1.5007122893333436, img_loss: 1.4189086081981659, reward_loss: 0.3565280531441376, discount_loss: 0.09999871253967285, kl_div: 0.000996399074792869\n",
      "Finished with score: -19.0\n",
      "Step 68000: Total Avg Loss: 1.513273652791977, img_loss: 1.4188156490325927, reward_loss: 0.3223906457840121, discount_loss: 0.2997987151145935, kl_div: 1.4901164746561336e-11\n",
      "Step 68500: Total Avg Loss: 1.4938947014808655, img_loss: 1.4188342914581298, reward_loss: 0.3253026807755198, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 69000: Total Avg Loss: 1.5641785809993745, img_loss: 1.418810220003128, reward_loss: 0.5769424320847993, discount_loss: 0.2997987151145935, kl_div: 2.60770365301255e-11\n",
      "Step 69500: Total Avg Loss: 1.5171267466545104, img_loss: 1.4188643136024475, reward_loss: 0.441312805944468, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 70000: Total Avg Loss: 1.579489405155182, img_loss: 1.4189675381183624, reward_loss: 0.6502189860507651, discount_loss: 0.2997987151145935, kl_div: 0.0009963990449905395\n",
      "Step 70500: Total Avg Loss: 1.54727800989151, img_loss: 1.4188990762233735, reward_loss: 0.5918953110511102, discount_loss: 0.09999871253967285, kl_div: 6.332994928470725e-11\n",
      "Step 71000: Total Avg Loss: 1.4992939889431, img_loss: 1.4188256716728211, reward_loss: 0.3523422253631661, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 71500: Total Avg Loss: 1.5438514926433564, img_loss: 1.4189275164604187, reward_loss: 0.4747205183586897, discount_loss: 0.2997987151145935, kl_div: 7.450581485102248e-12\n",
      "Step 72000: Total Avg Loss: 1.5087912373542787, img_loss: 1.418830615758896, reward_loss: 0.39980375496997844, discount_loss: 0.09999871253967285, kl_div: 1.4901162970204496e-11\n",
      "Step 72500: Total Avg Loss: 1.5272958731651307, img_loss: 1.4188714144229888, reward_loss: 0.48963193410358874, discount_loss: 0.09999871253967285, kl_div: 0.0009963990487158298\n",
      "Finished with score: -19.0\n",
      "Step 73000: Total Avg Loss: 1.571139181137085, img_loss: 1.4188091719150544, reward_loss: 0.6117506903527989, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 73500: Total Avg Loss: 1.533054966688156, img_loss: 1.4190089738368987, reward_loss: 0.5202306081993029, discount_loss: 0.09999871253967285, kl_div: 7.450581485102248e-12\n",
      "Finished with score: -19.0\n",
      "Step 74000: Total Avg Loss: 1.5345464532375335, img_loss: 1.418843493461609, reward_loss: 0.4286154268435805, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 74500: Total Avg Loss: 1.4930991652011871, img_loss: 1.4188109912872315, reward_loss: 0.3214415070897047, discount_loss: 0.09999871253967285, kl_div: 7.450581485102248e-12\n",
      "Step 75000: Total Avg Loss: 1.5142189400196076, img_loss: 1.4188947269916534, reward_loss: 0.42662170233895913, discount_loss: 0.09999871253967285, kl_div: 3.725290298461914e-12\n",
      "Finished with score: -21.0\n",
      "Step 75500: Total Avg Loss: 1.5623354926109314, img_loss: 1.4188711030483245, reward_loss: 0.5649315890802686, discount_loss: 0.2997987151145935, kl_div: 0.0009963990561664131\n",
      "Step 76000: Total Avg Loss: 1.5123066091537476, img_loss: 1.4188574209213256, reward_loss: 0.4147555902886013, discount_loss: 0.09999871253967285, kl_div: 0.0009963990524411211\n",
      "Finished with score: -19.0\n",
      "Step 76500: Total Avg Loss: 1.5960240411758422, img_loss: 1.418817673444748, reward_loss: 0.736132484680184, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 77000: Total Avg Loss: 1.5043259711265564, img_loss: 1.418817666053772, reward_loss: 0.37754215668884716, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Step 77500: Total Avg Loss: 1.5440773663520813, img_loss: 1.4189359986782073, reward_loss: 0.5757074941878527, discount_loss: 0.09999871253967285, kl_div: 1.4901163858382916e-11\n",
      "Finished with score: -19.0\n",
      "Step 78000: Total Avg Loss: 1.5280408549308777, img_loss: 1.4188179080486298, reward_loss: 0.3962153609469606, discount_loss: 0.2997987151145935, kl_div: 3.725290298461914e-12\n",
      "Step 78500: Total Avg Loss: 1.4966212451457976, img_loss: 1.4188341374397277, reward_loss: 0.33893618176682344, discount_loss: 0.09999871253967285, kl_div: 1.4901164746561336e-11\n",
      "Finished with score: -20.0\n",
      "Step 79000: Total Avg Loss: 1.5201865413188935, img_loss: 1.4188432137966156, reward_loss: 0.3568172859293918, discount_loss: 0.2997987151145935, kl_div: 1.1175873559921002e-11\n",
      "Step 79500: Total Avg Loss: 1.4791897187232972, img_loss: 1.4188469791412353, reward_loss: 0.2517143422230711, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 80000: Total Avg Loss: 1.5156305069923401, img_loss: 1.418902717113495, reward_loss: 0.33373960854558304, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 80500: Total Avg Loss: 1.502291220664978, img_loss: 1.418808438539505, reward_loss: 0.3674145428865729, discount_loss: 0.09999871253967285, kl_div: 1.862645504502325e-11\n",
      "Step 81000: Total Avg Loss: 1.490654114484787, img_loss: 1.4188290450572967, reward_loss: 0.30912600133081425, discount_loss: 0.09999871253967285, kl_div: 1.4901162970204496e-11\n",
      "Finished with score: -21.0\n",
      "Step 81500: Total Avg Loss: 1.5148700451850892, img_loss: 1.4188076634407043, reward_loss: 0.33041256174159706, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 82000: Total Avg Loss: 1.521990452528, img_loss: 1.4189953727722169, reward_loss: 0.46497604272165516, discount_loss: 0.09999871253967285, kl_div: 3.725292074818753e-11\n",
      "Step 82500: Total Avg Loss: 1.5381683130264283, img_loss: 1.4188100469112397, reward_loss: 0.544340414952869, discount_loss: 0.09999871253967285, kl_div: 0.0009806280732154846\n",
      "Finished with score: -19.0\n",
      "Step 83000: Total Avg Loss: 1.5663225071430207, img_loss: 1.4188143253326415, reward_loss: 0.5876415576760748, discount_loss: 0.2997987151145935, kl_div: 3.725290298461914e-12\n",
      "Step 83500: Total Avg Loss: 1.528790691137314, img_loss: 1.4188763835430145, reward_loss: 0.4995721720422918, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -19.0\n",
      "Step 84000: Total Avg Loss: 1.5527536523342134, img_loss: 1.418843175649643, reward_loss: 0.5196530052252588, discount_loss: 0.2997987151145935, kl_div: 7.450581485102248e-12\n",
      "Step 84500: Total Avg Loss: 1.4992012813091278, img_loss: 1.4188267509937287, reward_loss: 0.3518732872018554, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Step 85000: Total Avg Loss: 1.5151580483913423, img_loss: 1.4188443841934204, reward_loss: 0.43156896537118894, discount_loss: 0.09999871253967285, kl_div: 1.1175873559921002e-11\n",
      "Finished with score: -20.0\n",
      "Step 85500: Total Avg Loss: 1.5298136410713197, img_loss: 1.418849453687668, reward_loss: 0.4049215817474533, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 86000: Total Avg Loss: 1.4742978122234345, img_loss: 1.418810643672943, reward_loss: 0.22743648696467914, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Step 86500: Total Avg Loss: 1.514972440481186, img_loss: 1.4188553102016448, reward_loss: 0.4305862840677146, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -19.0\n",
      "Step 87000: Total Avg Loss: 1.5027892286777496, img_loss: 1.4188135771751405, reward_loss: 0.2699788901648764, discount_loss: 0.2997987151145935, kl_div: 1.4901164746561336e-11\n",
      "Step 87500: Total Avg Loss: 1.5274409747123718, img_loss: 1.4190725722312927, reward_loss: 0.49184265489174384, discount_loss: 0.09999871253967285, kl_div: 1.862645504502325e-11\n",
      "Finished with score: -20.0\n",
      "Step 88000: Total Avg Loss: 1.5095230989456176, img_loss: 1.4188608219623566, reward_loss: 0.30341202255808053, discount_loss: 0.2997987151145935, kl_div: 1.862645504502325e-11\n",
      "Step 88500: Total Avg Loss: 1.5041213912963867, img_loss: 1.4188086533546447, reward_loss: 0.3765643333565546, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 89000: Total Avg Loss: 1.5236199343204497, img_loss: 1.4188676199913024, reward_loss: 0.3738622260574557, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 89500: Total Avg Loss: 1.500964016199112, img_loss: 1.4188212335109711, reward_loss: 0.3607145649227441, discount_loss: 0.09999871253967285, kl_div: 7.450581485102248e-12\n",
      "Step 90000: Total Avg Loss: 1.5181873867511748, img_loss: 1.4188555040359496, reward_loss: 0.4466600570740396, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 90500: Total Avg Loss: 1.5142920484542848, img_loss: 1.4188085927963257, reward_loss: 0.32751791338003516, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 91000: Total Avg Loss: 1.496864313364029, img_loss: 1.4188314020633697, reward_loss: 0.34016518759672065, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -20.0\n",
      "Step 91500: Total Avg Loss: 1.5140192792415619, img_loss: 1.4188618211746216, reward_loss: 0.3258879251858264, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 92000: Total Avg Loss: 1.5161415235996247, img_loss: 1.4188101353645324, reward_loss: 0.4366575871787529, discount_loss: 0.09999871253967285, kl_div: 7.450581485102248e-12\n",
      "Step 92500: Total Avg Loss: 1.5446091237068176, img_loss: 1.4188336491584779, reward_loss: 0.5788780114454186, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 93000: Total Avg Loss: 1.5566175456047058, img_loss: 1.4188164067268372, reward_loss: 0.5366153408820428, discount_loss: 0.2997987151145935, kl_div: 0.0009963990524411211\n",
      "Step 93500: Total Avg Loss: 1.5067287738323212, img_loss: 1.418815883398056, reward_loss: 0.3895650827339127, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -21.0\n",
      "Step 94000: Total Avg Loss: 1.520759595155716, img_loss: 1.4188362982273102, reward_loss: 0.3597171191930802, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 94500: Total Avg Loss: 1.491677721977234, img_loss: 1.4188324265480041, reward_loss: 0.31422712843005957, discount_loss: 0.09999871253967285, kl_div: 3.725290298461914e-12\n",
      "Finished with score: -21.0\n",
      "Step 95000: Total Avg Loss: 1.5266813118457794, img_loss: 1.4188252935409547, reward_loss: 0.38938073129303064, discount_loss: 0.2997987151145935, kl_div: 1.4901164746561336e-11\n",
      "Step 95500: Total Avg Loss: 1.5236904809474945, img_loss: 1.4189249033927918, reward_loss: 0.47382852395241387, discount_loss: 0.09999871253967285, kl_div: 7.450581485102248e-12\n",
      "Step 96000: Total Avg Loss: 1.5409909055233002, img_loss: 1.4188159410953523, reward_loss: 0.5608754566055999, discount_loss: 0.09999871253967285, kl_div: 5.960468030252741e-11\n",
      "Finished with score: -20.0\n",
      "Step 96500: Total Avg Loss: 1.5139183735847472, img_loss: 1.4188132202625274, reward_loss: 0.3256264082099887, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 97000: Total Avg Loss: 1.498011269569397, img_loss: 1.4188240313529967, reward_loss: 0.3459368425878476, discount_loss: 0.09999871253967285, kl_div: 1.1175873559921002e-11\n",
      "Step 97500: Total Avg Loss: 1.5133802993297576, img_loss: 1.4188607308864594, reward_loss: 0.4225984766144515, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -18.0\n",
      "Step 98000: Total Avg Loss: 1.5508375899791718, img_loss: 1.4188437707424164, reward_loss: 0.5100697338774862, discount_loss: 0.2997987151145935, kl_div: 7.450581485102248e-12\n",
      "Step 98500: Total Avg Loss: 1.4949994564056397, img_loss: 1.418803406715393, reward_loss: 0.3309808932894506, discount_loss: 0.09999871253967285, kl_div: 0.0\n",
      "Finished with score: -20.0\n",
      "Step 99000: Total Avg Loss: 1.5589016220569611, img_loss: 1.4188114445209503, reward_loss: 0.5505515307600287, discount_loss: 0.2997987151145935, kl_div: 0.0\n",
      "Step 99500: Total Avg Loss: 1.5158294749259948, img_loss: 1.4189073996543884, reward_loss: 0.43461100931035435, discount_loss: 0.09999871253967285, kl_div: 1.1175873559921002e-11\n",
      "Finished with score: -21.0\n",
      "Step 100000: Total Avg Loss: 1.5053400566577912, img_loss: 1.4188889381885528, reward_loss: 0.2823562272305662, discount_loss: 0.2997987151145935, kl_div: 0.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions.kl import kl_divergence\n",
    "from torchvision.transforms import Resize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from world_model import WorldModel\n",
    "\n",
    "seed = 5\n",
    "env = gym.make(\"Pong-v0\")\n",
    "env.seed(seed)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "world_model = WorldModel(device)\n",
    "optimizer = optim.Adam(world_model.parameters(), lr=0.001)\n",
    "\n",
    "img = torch.tensor(env.reset()).unsqueeze(0)\n",
    "# env.render()\n",
    "total_reward = 0\n",
    "total_loss = 0\n",
    "total_losses = np.zeros(4)\n",
    "check_iters = 500\n",
    "avg_losses = []\n",
    "for i in range(1, 100001):\n",
    "    action = np.random.randint(env.action_space.n)\n",
    "    next_img, true_reward, done, info = env.step(action)\n",
    "    # env.render()\n",
    "    true_discount = 0.999 if not done else 0.0\n",
    "    next_img = torch.tensor(next_img).unsqueeze(0)\n",
    "    total_reward += true_reward\n",
    "\n",
    "    hidden = torch.zeros(1, world_model.n_hidden).to(device)\n",
    "    prior, posterior, gen_img, std_img, reward, discount = world_model.get_real_states(img, hidden)\n",
    "    hidden = world_model.get_next_hidden(hidden, posterior, action)\n",
    "    loss, losses = world_model.calc_loss(prior, posterior, gen_img, std_img, reward, true_reward, discount, true_discount)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(world_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    total_losses += losses\n",
    "\n",
    "    total_loss += float(loss.detach().cpu())\n",
    "    if i % check_iters == 0:\n",
    "        avg_loss = total_loss / check_iters\n",
    "        avg_losses_array = total_losses / check_iters\n",
    "        print(\n",
    "            f\"Step {i}: Total Avg Loss: {avg_loss}, img_loss: {avg_losses_array[0]}, \"\n",
    "            f\"reward_loss: {avg_losses_array[1]}, discount_loss: {avg_losses_array[2]},\"\n",
    "            f\" kl_div: {avg_losses_array[3]}\"\n",
    "        )\n",
    "        avg_losses.append(avg_loss)\n",
    "        total_loss = 0\n",
    "        total_losses = np.zeros(4)\n",
    "\n",
    "    if done:\n",
    "        print(f\"Finished with score: {total_reward}\")\n",
    "        total_reward = 0\n",
    "        img = torch.tensor(env.reset()).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1626748633315,
     "user": {
      "displayName": "Tony Schroeder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilICEJmMvke-YcAXapHV2CDnsfjBfDiGICD336=s64",
      "userId": "03301841124230681313"
     },
     "user_tz": 420
    },
    "id": "zbEFSl5PuC6w",
    "outputId": "4ac5be0b-e63e-4aad-d574-2d8aaaaf3cd4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f564e232e10>]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyLKynn4XX-i",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOGIrg91d4/77CZtwYqleea",
   "collapsed_sections": [],
   "name": "dreamerv2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}