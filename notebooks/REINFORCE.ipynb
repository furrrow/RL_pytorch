{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Implementation of the REINFORCE algorithm\n",
    "\n",
    "From Miguel's Book, combined with my old code.\n",
    "\n",
    "https://github.com/mimoralea/gdrl/blob/master/notebooks/chapter_11/chapter-11.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# will take some of miguel's import statements, but not all.\n",
    "import gym\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "action space: Discrete(2)\n",
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "input shape: (4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([-0.01358154, -0.01906393, -0.02498913, -0.02085484], dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", device)\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "n_action = env.action_space.n\n",
    "n_states = env.observation_space.shape[0]\n",
    "input_shape= env.observation_space.shape\n",
    "print(\"action space:\", env.action_space)\n",
    "print(\"observation space:\", env.observation_space)\n",
    "print(\"input shape:\", input_shape)\n",
    "env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DenseNetwork(nn.Module):\n",
    "    def __init__(self, in_features,\n",
    "                 out_features,\n",
    "                 hidden_features=(32, 32),\n",
    "                 activation = F.relu):\n",
    "        super(DenseNetwork, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.input_layer = nn.Linear(in_features, hidden_features[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_features - 1)):\n",
    "            hidden_layer = nn.Linear(hidden_features[i], hidden_features[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_features[-1], out_features)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x,\n",
    "                             device=self.device,\n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self._format(state)\n",
    "        state = self.activation(self.input_layer(state))\n",
    "        for layer in self.hidden_layers:\n",
    "            state = self.activation(layer(state))\n",
    "        state = self.activation(self.output_layer(state))\n",
    "        return state\n",
    "\n",
    "    def full_pass(self, state):\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        logpa = dist.log_prob(action).unsqueeze(-1)\n",
    "        entropy = dist.entropy().unsqueeze(-1)\n",
    "        is_exploratory = action != np.argmax(logits.detach().numpy())\n",
    "        return action.item(), is_exploratory.item(), logpa, entropy\n",
    "\n",
    "    def select_action(self, state):\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def select_greedy_action(self, state):\n",
    "        logits = self.forward(state)\n",
    "        return np.argmax(logits.detach().numpy())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self, policy_model_fn, policy_optimizer_fn, policy_optimizer_lr):\n",
    "        self.n_action = None\n",
    "        self.n_states = None\n",
    "        self.policy_optimizer = None\n",
    "        self.policy_model = None\n",
    "        self.env = None\n",
    "        self.gamma = None\n",
    "        self.seed = None\n",
    "        self.make_env_kargs = None\n",
    "        self.make_env_fn = None\n",
    "        self.logpas = None\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "        self.episode_reward = []\n",
    "        self.iterator = count(start=0, step=1)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        T = len(self.rewards)\n",
    "        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n",
    "        returns = np.array([np.sum(discounts[:T-t] * self.rewards[t:]) for t in range(T)])\n",
    "\n",
    "        discounts = torch.FloatTensor(discounts).unsqueeze(1)\n",
    "        returns = torch.FloatTensor(returns).unsqueeze(1)\n",
    "        self.logpas = torch.cat(self.logpas)\n",
    "\n",
    "        policy_loss = -(discounts * returns * self.logpas).mean()\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state):\n",
    "        action, is_exploratory, logpa, _ = self.policy_model.full_pass(state)\n",
    "        new_state, reward, is_terminal, _ = self.env.step(action)\n",
    "\n",
    "        self.logpas.append(logpa)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += int(is_exploratory)\n",
    "\n",
    "    def train(self, env, gamma, max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.n_states, self.n_action = env.observation_space.shape[0], env.action_space.n\n",
    "        self.policy_model = self.policy_model_fn(self.n_states, self.n_action)\n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model, self.policy_optimizer_lr)\n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        for episodes in range(max_episodes):\n",
    "            episode_start = time.time()\n",
    "            state, is_terminal = self.env.reset(), False\n",
    "            self.episode_reward.append(0.0)\n",
    "            next(self.iterator)\n",
    "\n",
    "            self.logpas, self.rewards = [], []\n",
    "            state, is_terminal = self.interaction_step(state)\n",
    "            if is_terminal:\n",
    "                gc.collect()\n",
    "                break\n",
    "\n",
    "            self.optimize_model()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}